{
  "cells": [
    {
      "metadata": {
        "_uuid": "31ce862a4caf5a38fe457908c211845315106f13",
        "_cell_guid": "95a0f564-0bc9-4e29-8985-a78150f95ff7"
      },
      "cell_type": "markdown",
      "source": "# TARUN SUNKARANENI'S Hierarchical and Point-Clustering Notebook"
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "collapsed": true,
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": false
      },
      "cell_type": "code",
      "source": "\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom scipy.spatial.distance import cdist\nfrom matplotlib import pyplot as plt\nfrom scipy.spatial import distance\nimport math\n%matplotlib inline\nnp.set_printoptions(precision=5, suppress=True)  # suppress scientific float notation\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "collapsed": true,
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": false
      },
      "cell_type": "code",
      "source": "c1 = pd.read_csv(\"input/C1.csv\", names=['x0', 'x1'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7986cbee0e499844dd23711172eb956bd8a140a8",
        "_cell_guid": "351e3587-c6f7-4828-9633-49b1ae351e21"
      },
      "cell_type": "markdown",
      "source": "# Single-Link Hierarchical\n### Single-Link: measures the shortest link \n$\\displaystyle{\\textbf{d}(S_1,S_2) = \\min_{(s_1,s_2) \\in S_1 \\times S_2} \\|s_1 - s_2\\|_2}$"
    },
    {
      "metadata": {
        "_uuid": "60dc1e4e062975f164ed9f77dc77b0a8cc95a486",
        "collapsed": true,
        "_cell_guid": "732be8e3-36e9-427f-aba1-6774c94fd632",
        "trusted": false
      },
      "cell_type": "code",
      "source": "plt.scatter(c1['x0'],c1['x1'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b51a1eec719d2c1f67aa6f3bdc294410ecc5a1d5",
        "collapsed": true,
        "_cell_guid": "ed08a12f-2e46-4a07-9db7-c393d0672d54",
        "trusted": false
      },
      "cell_type": "code",
      "source": "def single_distance(clusters ,cluster_num):\n    print('first cluster | ','second cluster | ', 'distance')\n    while len(clusters) is not cluster_num:\n        # Clustering           (\n        closest_distance=clust_1=clust_2 = math.inf\n        # for every cluster (until second last element)\n        for cluster_id, cluster in enumerate(clusters[:len(clusters)]): \n            # for each point in each cluster\n            for point_id,point in enumerate(cluster): \n                # we only need to compare with clusters after the current one\n                for cluster2_id, cluster2 in enumerate(clusters[(cluster_id+1):]): \n                    # go through every point in this prospective cluster as well\n                    for point2_id, point2 in enumerate(cluster2):\n# if this distance is better than our previous best distance then we are going to overwrite it\n                        if distance.euclidean(point,point2) < closest_distance: \n                            # Only used for comparing \n                            closest_distance = distance.euclidean(point,point2)\n                # this will be used at the end to figure out which cluster to merge with which\n                            clust_1 = cluster_id\n                # this cluster will be destroyed by the end\n                            clust_2 = cluster2_id+cluster_id+1\n               # extend just appends the contents to the list without flattening it out\n        print(clust_1,' | ',clust_2, ' | ',closest_distance)\n        clusters[clust_1].extend(clusters[clust_2]) \n        # don't need this index anymore, and we have just clustered once more\n        clusters.pop(clust_2) \n    return(clusters)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7ac6575fef403ea9354afa2956c82a954f78f1b1",
        "collapsed": true,
        "_cell_guid": "fcf579fe-4e4a-4d69-a92e-04119037ffa4",
        "trusted": false
      },
      "cell_type": "code",
      "source": "### Hierarchical clustering\ndef hierarchical(data, cluster_num, metric = 'single'):\n    # initialization of clusters at first (every point is a cluster)\n    init_clusters=[]\n    for index, row in data.iterrows():\n        init_clusters.append([[row['x0'], row['x1']]])\n    if metric is 'single':\n        return single_distance(init_clusters, cluster_num)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9d39dffabdd46bb2e36fae611cf78231ef7cabea",
        "collapsed": true,
        "_cell_guid": "d178b9b7-890b-4ad2-a458-bce05c7a09a2",
        "trusted": false
      },
      "cell_type": "code",
      "source": "clusters = hierarchical(c1,4)\ncolors = ['blue', 'red', 'purple', 'teal']\nfor cluster_index, cluster in enumerate(clusters):\n    for point_index, point in enumerate(cluster):\n        plt.plot([point[0]], [point[1]], marker='o', markersize=3, color=colors[cluster_index])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a45109da7f08f18942887a6342cccb7102c8562a",
        "_cell_guid": "37c15a51-988b-4e19-bff4-772e895ae373"
      },
      "cell_type": "markdown",
      "source": "> # Validation "
    },
    {
      "metadata": {
        "_uuid": "f85cdfc72ea66fb272725ed9fcae98cd13aa2ecb",
        "_cell_guid": "aefde9ae-a790-47ea-ad9f-bc32bf0f35c9"
      },
      "cell_type": "markdown",
      "source": "Credit to [https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/](http://) for this Validation portion"
    },
    {
      "metadata": {
        "_uuid": "8e48fe89504b6ef77441535a9b230673dac14794",
        "collapsed": true,
        "_cell_guid": "ff676b94-11b6-4cc2-a275-205a7940d37d",
        "trusted": false
      },
      "cell_type": "code",
      "source": "X = c1.as_matrix()\n# generate the linkage matrix\nsingle_link = linkage(X, 'single') # using single link metric to evaluate 'distance' between clusters",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "acf53a1520f601fdee9af85c9560c52d764f63ba",
        "_cell_guid": "c0162828-06ec-4f46-ba05-28bb8969dbb9"
      },
      "cell_type": "markdown",
      "source": " As you can see there's a lot of choice here and while python and scipy make it very easy to do the clustering, it's you who has to understand and make these choices.. This  compares the actual pairwise distances of all your samples to those implied by the hierarchical clustering. \n > The closer the value is to `1`, the better the clustering preserves the original distances, which in our case is reasonably close:"
    },
    {
      "metadata": {
        "_uuid": "6802058ae18b8002ce53f49d3c1410cf69453fac",
        "collapsed": true,
        "_cell_guid": "942dbb47-8294-4c31-823c-2178063886ee",
        "trusted": false
      },
      "cell_type": "code",
      "source": "from scipy.cluster.hierarchy import cophenet\nfrom scipy.spatial.distance import pdist",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c69eed9cdc59a4aa5aea86c63baec10c9aa3587f",
        "collapsed": true,
        "_cell_guid": "894af22d-3b90-46ca-9cea-fa9a66d4bd9c",
        "trusted": false
      },
      "cell_type": "code",
      "source": "c, coph_dists = cophenet(single_link, pdist(X))\nc",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cb8092526b39fcd25d49c25a3446fcbfbcda79e3",
        "_cell_guid": "fda7583d-1474-47c5-b2ed-b93e6f1b5e1d"
      },
      "cell_type": "markdown",
      "source": "No matter what method and metric you pick, the linkage() function will use that method and metric to calculate the distances of the clusters (starting with your n individual samples (aka data points) as singleton clusters)) and in each iteration will merge the two clusters which have the smallest distance according the selected method and metric. It will return an array of length `n - 1` giving you information about the `n - 1` cluster merges which it needs to pairwise merge n clusters. `single_link[i]` will tell us which clusters were merged in the i-th iteration, let's take a look at the first two points that were merged:"
    },
    {
      "metadata": {
        "_uuid": "a284d47c714e72d37e8c55dd0088dc066e797175",
        "collapsed": true,
        "_cell_guid": "5d04913a-12fb-47a0-8952-a908b6d013dd",
        "trusted": false
      },
      "cell_type": "code",
      "source": "single_link[0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b98b22bb6ec6f5f0d4c419cc6b21acb67a523be9",
        "_cell_guid": "08cdbd35-4914-4917-94b2-dcd33fafdf21"
      },
      "cell_type": "markdown",
      "source": "In its first iteration the linkage algorithm decided to merge the two clusters  with indices `2` and `3`, as they only had a distance of `0.15085`. This created a cluster with a total of `2` samples. \n> We can see that each row of the resulting array has the format `[idx1, idx2, dist, sample_count].`\n\n\n"
    },
    {
      "metadata": {
        "_uuid": "ded6e0655cb223daf3002b108878cb38598d6254",
        "collapsed": true,
        "_cell_guid": "e2716e17-9819-4507-9974-61c8df7d8b59",
        "trusted": false
      },
      "cell_type": "code",
      "source": "single_link[1]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "334e640a4e9470a106efed5ae0e7990c5885bbe9",
        "_cell_guid": "27543935-ec8e-4820-acbf-3f638af6a7ae"
      },
      "cell_type": "markdown",
      "source": "In the second iteration the algorithm decided to merge the clusters (original samples here as well) with indices `16` and `17`, which had a distance of `0.15501`. This again formed another cluster with a total of `2` samples.\n\nThe indices of the clusters until now correspond to our samples. Remember that we had a total of 21 samples, so indices `0` to `20`. Let's have a look at the first `20` iterations:"
    },
    {
      "metadata": {
        "_uuid": "e9e7269195fd0c7f050d77362f2925b4f380f2f3",
        "collapsed": true,
        "_cell_guid": "ff96fc25-6429-4540-bc95-a47246d66e58",
        "trusted": false
      },
      "cell_type": "code",
      "source": "single_link[:20]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1e33e53f1098a7add3aea9053b955e8ac2175862",
        "_cell_guid": "0e71367a-c7a0-49e5-866b-35ff2dc0b84f"
      },
      "cell_type": "markdown",
      "source": "We can  observe the monotonic increase of the distance. This is also similar to the results we had with our run. Note that out algorithm with `4` clusters ends at the distance of `0.47` ish, whereas the above information pertains to finishing with `1` cluster.\n\nOur Output:\n\n`first cluster |  second cluster |  distance\n2  |  3  |  0.15085042956518227\n15  |  16  |  0.15501250939640307\n15  |  16  |  0.155332720189051\n14  |  15  |  0.16302722103464817\n14  |  15  |  0.1679299964569166\n13  |  14  |  0.17501291697817623\n12  |  13  |  0.18766276935111553\n4  |  5  |  0.19186599490269243\n11  |  12  |  0.19429172919524904\n10  |  11  |  0.19888079280176854\n5  |  6  |  0.20597708099371148\n8  |  9  |  0.20743622990434923\n5  |  6  |  0.21188849347451605\n6  |  7  |  0.2126411284874588\n5  |  6  |  0.21415518261316957\n3  |  4  |  0.22340895237210173\n0  |  1  |  0.47931424457263944`"
    },
    {
      "metadata": {
        "_uuid": "648166977529ccb31d4d301655f5f76c13bf55f8",
        "_cell_guid": "c23918b0-4359-45ea-a680-7a9caa57b9bd"
      },
      "cell_type": "markdown",
      "source": "## Dendogram\n> A dendrogram is a visualization in form of a tree showing the order and distances of merges during the hierarchical clustering."
    },
    {
      "metadata": {
        "_uuid": "3f11903f03aa1dd062f415abf2818643984c2520",
        "collapsed": true,
        "_cell_guid": "551e1655-5fca-4ee5-8496-0bca0f88e4f9",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# calculate full dendrogram\nplt.figure(figsize=(25, 10))\nplt.title('Hierarchical Clustering Dendrogram')\nplt.xlabel('sample index')\nplt.ylabel('distance')\ndendrogram(\n    single_link,\n    leaf_rotation=90.,  # rotates the x axis labels\n    leaf_font_size=8.,  # font size for the x axis labels\n    color_threshold= .6\n)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d18e8a9fb385e496c3ac60ed3382b8dd3b92c149",
        "_cell_guid": "46453f4a-2a5a-487e-90a1-05b02c07eefb"
      },
      "cell_type": "markdown",
      "source": "Which actually corresponds to our results as well (pasted from top again)"
    },
    {
      "metadata": {
        "_uuid": "5f6a86e038f60f9160fe22e95a0b47ab089949a9",
        "collapsed": true,
        "_cell_guid": "e9f50e04-3a22-4934-9a48-95515eacd96d",
        "trusted": false
      },
      "cell_type": "code",
      "source": "clusters = hierarchical(c1,4)\ncolors = ['blue', 'red', 'purple', 'teal']\nfor cluster_index, cluster in enumerate(clusters):\n    for point_index, point in enumerate(cluster):\n        plt.plot([point[0]], [point[1]], marker='o', markersize=3, color=colors[cluster_index])",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "name": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}